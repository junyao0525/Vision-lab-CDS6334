{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT_hEdvXvkPk"
   },
   "source": [
    "# Lab08 - Interest Point Detection and Description\n",
    "### CDS6334 Visual Information Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpiw3s71vkPp"
   },
   "source": [
    "This lab explores the creation of local invariant features through two steps: detection of interest points (or keypoints), and the description of local features from the extracted interest points. A number of functions to perform these tasks are readily available from OpenCV, while some are now available from the unofficial OpenCV contrib modules which can be easily installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiTg41sHvkPt"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXSssO3lvkP_"
   },
   "source": [
    "## Interest point detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2AaRN2avkQB"
   },
   "source": [
    "### Harris corner detector\n",
    "\n",
    "The Harris corner detector is the most classic interest point detector which aims to find corners in an image. Corners are said to be very good candidates for interest points as they possess properties of distinctiveness (interesting and unique to the image) and repeatability (easily found again despite transformations). <br>\n",
    "The measure of cornerness is given by the equation:\n",
    "\n",
    "$$R = det(M) - k(trace(M))^2$$\n",
    "\n",
    "where\n",
    "* $M = \\sum \\limits_{x,y}w(x,y)\\begin{bmatrix}\n",
    "I_xI_x & I_xI_y \\\\\n",
    "I_xI_y & I_yI_y\n",
    "\\end{bmatrix}$\n",
    "* $det(M) = \\lambda_{1} \\lambda_{2}$\n",
    "* $trace(M) = \\lambda_{1} + \\lambda_{2}$\n",
    "* $\\lambda_{1}$ and $\\lambda_{2}$ are the eigenvalues of $M$\n",
    "\n",
    "The function `cv2.cornerHarris(img, blocksize, ksize, k)` has the following arguments:\n",
    "\n",
    "- *img* - Input image, it should be grayscale and float32 type.\n",
    "- *blockSize* - It is the size of neighbourhood considered for corner detection\n",
    "- *ksize* - Aperture parameter of Sobel derivative used, basically the mask size\n",
    "- *k* - Harris detector free parameter in the equation.\n",
    "\n",
    "It returns an output with the same shape as the input image (`img`) where the values represent the *cornerness* of the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W31cWbsuvkQE"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('house.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray, cmap='gray'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,3,3,0.04)\n",
    "\n",
    "# you can turn this on to make the corners more obvious\n",
    "dst = cv2.dilate(dst,None)     \n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-seXwywUvkQV"
   },
   "source": [
    "**Q1**: Study the re-implemented Harris Corner Detector function given (7 lines only!). Modify it further to take the top *N* corners, as determined by the cornerness measure. This way, we can take only the *N* strongest corners in the image (indirecty, this also removes the need to threshold the found corners!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2MQqa9nvkQX"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('house.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Write the function of your modified Harris corner detector here\n",
    "def modifiedHarris(gray, blockSize, ksz, k):\n",
    "    Ix = cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=ksz)\n",
    "    Iy = cv2.Sobel(gray,cv2.CV_64F,0,1,ksize=ksz)\n",
    "    Ix2 = cv2.GaussianBlur(Ix*Ix,(blockSize,blockSize),0)\n",
    "    Iy2 = cv2.GaussianBlur(Iy*Iy,(blockSize,blockSize),0)\n",
    "    Ixy = cv2.GaussianBlur(Ix*Iy,(blockSize,blockSize),0)\n",
    "    cornerness = (Ix2*Iy2 - Ixy*Ixy) - k*((Ix2 + Iy2)**2);    # det(A) - k*[trace(A)^2]\n",
    "    ###Add code here to rank and return only N strongest corners\n",
    "    \n",
    "    ###\n",
    "    return cornerness\n",
    "\n",
    "c = modifiedHarris(gray, 3, 3, 0.04)   # same parameters as OpenCV's function\n",
    "img[c>0.01*c.max()]=[0,0,255]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKZUpzIRvkQk"
   },
   "source": [
    "### Refining corners\n",
    "\n",
    "Sometimes, you may need to find the corners that are \"more exact\". OpenCV comes with a function `cv2.cornerSubPix()` which further refines the corners detected with sub-pixel accuracy (going beyond pixel coordinates to find \"true corners\").\n",
    "\n",
    "First, find the Harris corners. Then, pass the centroids of these corners (There may be a bunch of pixels at a corner, we take their centroid) to refine them. We mark the Harris corners in red pixels while the refined corners are marked in green pixels. Also, we have to define the criteria to stop the iteration. We stop it after a specified number of iteration or a certain accuracy is achieved, whichever occurs first. We also need to define the size of neighbourhood it would search for corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze-JG68YvkQm"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('house.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find Harris corners\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "# this dilation step is useful to \"grow\" the corners so that they overlap with other nearby corners.\n",
    "# then, we can refine the corner with better consideration\n",
    "ori_dst = dst\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# threshold\n",
    "ori_nCorners = np.sum(ori_dst>0.01*ori_dst.max())\n",
    "ret, dst = cv2.threshold(dst,0.01*dst.max(),255,0)\n",
    "dst = np.uint8(dst)\n",
    "\n",
    "# find centroids\n",
    "ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "\n",
    "# define the criteria to stop and refine the corners\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv2.cornerSubPix(gray,np.float32(centroids),(5,5),(-1,-1),criteria)\n",
    "\n",
    "# draw the original corners\n",
    "img2 = cv2.imread('house.png')    # read again instead of making copies of the array 'img'\n",
    "img2[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "# draw the refined corners\n",
    "res = np.hstack((centroids,corners))\n",
    "res = np.intp(res)\n",
    "\n",
    "# draw bigger marker at the corners for better clarity        \n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        img[res[:,1]+i-1,res[:,0]+j-1]=[255,0,0]\n",
    "        img[res[:,3]+i-1,res[:,2]+j-1] = [0,255,0]\n",
    "                \n",
    "        \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121), plt.imshow(img2), plt.title('Original Harris corners (blue)')\n",
    "plt.subplot(122), plt.imshow(img), plt.title('Refined Harris corners (green)')\n",
    "plt.show()\n",
    "print('Number of corners before refinement: %d'%(ori_nCorners))\n",
    "print('Number of corners after refinement: %d'%(res.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzyZElCQvkQx"
   },
   "source": [
    "Now that this refinement step helps to reduce the number of redundant corners (that are too close to a particular true corner), we can now revisit the earlier task on taking the top N corners, which will reduce further the number of refined corners to a much smaller number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzwry3_rvkQz"
   },
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kn7vvNKvkQ1"
   },
   "source": [
    "Let's turn our attention to the SIFT detector and descriptor. Unfortunately, for OpenCV 3.0 and later, SIFT and a few other feature methods (such as SURF) have been removed since they are considered \"non-free\" patented algorithms. The unofficial [OpenCV contrib modules](https://pypi.python.org/pypi/opencv-contrib-python) can now be installed through conda:\n",
    "\n",
    "**`pip install opencv-contrib-python`**\n",
    "\n",
    "Then, you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEiU_XEGvkQ4"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('notredame1.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray= cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# this creates an xfeatures2d_SIFT object, doesn't do anything yet\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "print(type(sift))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb1RooWJvkRF"
   },
   "source": [
    "Documentation is difficult to find and may be confusing to beginners! A majority of feature extractors are residing under [`feature2D`](https://docs.opencv.org/3.4.1/d0/d13/classcv_1_1Feature2D.html) class; over there, those that are from the OpenCV contrib modules can be found under [`xfeatures2d`](https://docs.opencv.org/3.4.1/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html) class where SIFT resides. The function to detect SIFT interest points and compute its descriptors is [`detectAndCompute()`](https://docs.opencv.org/master/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wK0-eDjvkRH"
   },
   "outputs": [],
   "source": [
    "# detects and computes SIFT keypoints and descriptors\n",
    "(kps, descs) = sift.detectAndCompute(gray, None)\n",
    "\n",
    "print(len(kps))        # number of SIFT keypoints\n",
    "print(descs.shape)     # shape of descriptor matrix. N keypoints x D dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynK6dVoXvkRV"
   },
   "source": [
    "This tells us that there are altogether 2,337 SIFT keypoints detected, and it resulted in the same number of descriptors, each being 128-dimension (basically 128 values describing the feature). \n",
    "\n",
    "To set other parameters related to SIFT, you have to do it earlier in the `SIFT_create()` function. \n",
    "- The *contrast threshold* is used to filter out weak features in low-contrast regions. The larger the threshold, the less features are produced by the detector.\n",
    "- The *edgeThreshold* is used to filter out edge-like features. The larger the threshold, the less features are filtered out (more features are retained). \n",
    "- There are 3 more parameters: *nFeatures*, *nOctaveLayers* and *sigma* that can be used to alter how SIFT works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSORSNNWvkRY"
   },
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.14, edgeThreshold=5)\n",
    "(kps, descs) = sift.detectAndCompute(gray, None)\n",
    "print(len(kps))\n",
    "print(descs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO7o3GgTvkRi"
   },
   "source": [
    "Now we have much lesser number of keypoints/descriptors, due to a higher *contrastThreshold* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG6nFTRMvkRl"
   },
   "outputs": [],
   "source": [
    "print(kps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZaEhT1UvkRv"
   },
   "source": [
    "What you see are *KeyPoint* objects. OpenCV is big on descriptors, they have even a dedicated object type just for the purpose of handling interest points! Documentation for Keypoint [here](https://docs.opencv.org/master/d2/d29/classcv_1_1KeyPoint.html).\n",
    "\n",
    "A popular feature library made famous in Matlab called VLFeat (we don't use it here), has a descriptor plotting function that can show the SIFT patches at different scales and orientations, which looks very cool:\n",
    "![plotdescriptor](http://www.vlfeat.org/demo/sift_basic_3.jpg)\n",
    "\n",
    "For us to visualize these keypoints, we can opt to draw a circle centered upon each keypoint with its radius indicating the size of the keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tjy7D7TwvkRx"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(img)\n",
    "fig = plt.gcf()\n",
    "ax = fig.gca()\n",
    "\n",
    "for r in np.arange(len(kps)): \n",
    "    circle1 = plt.Circle((kps[r].pt[0], kps[r].pt[1]), kps[r].size/2, color='r', fill=False)    \n",
    "    ax.add_artist(circle1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elNYO1uNvkR-"
   },
   "source": [
    "Another way to visualize the SIFT keypoints is to use the `cv2.drawKeypoints()` function. For this, we need to create KeyPoint objects to store the location, size (radius) and orientation of the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1VmlE7AvkSA"
   },
   "outputs": [],
   "source": [
    "imm = np.zeros((img.shape))    \n",
    "imm = cv2.drawKeypoints(img, kps, imm, flags=4);\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(imm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toTo_bfEvkSJ"
   },
   "source": [
    "**Q2**: Extract the top 200 features from SIFT and display them. You may have to relax on the contrast threshold (bring it down) to generate more keypoints. Compare them with the previous ones. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mNN60cRvkSL"
   },
   "outputs": [],
   "source": [
    "#Enter code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K640KveBvkST"
   },
   "source": [
    "Extract SIFT again for the second Notre Dame image. For consistency, make sure the parameters used are the same as the SIFTs extracted from the first Notre Dame image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29p4MfzyvkSV"
   },
   "outputs": [],
   "source": [
    "img1 = cv2.imread('notredame1.jpg')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "gray1= cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "sift1 = cv2.xfeatures2d.SIFT_create()\n",
    "(kps1, des1) = sift1.detectAndCompute(gray1, None)\n",
    "print(des1.shape)\n",
    " \n",
    "img2 = cv2.imread('notredame2.jpg')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "gray2= cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "sift2 = cv2.xfeatures2d.SIFT_create()\n",
    "(kps2, des2) = sift2.detectAndCompute(gray2, None)\n",
    "print(des2.shape)    \n",
    "\n",
    "imm1 = np.zeros((img1.shape))    \n",
    "imm1 = cv2.drawKeypoints(img1, kps1, imm1, flags=4);   \n",
    "imm2 = np.zeros((img2.shape))    \n",
    "imm2 = cv2.drawKeypoints(img2, kps2, imm2, flags=4);   \n",
    "\n",
    "plt.imshow(imm1), plt.title('notre dame 1')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "plt.imshow(imm2), plt.title('notre dame 2')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ixEGZ7PvkSg"
   },
   "source": [
    "Both Notre Dames have different number of descriptors. That's alright, this is normal when using SIFT. Because SIFT is dependent on the image content, it always produces different number of descriptors, even with the same parameter settings. More complex image content normally generates more interest points, hence more descriptors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Fz08y3vkSi"
   },
   "source": [
    "### Matching SIFT descriptors between images\n",
    "\n",
    "OpenCV's Brute-Force matcher can help us match descriptors between images. How it works is really simple. It takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. And the closest one is returned.\n",
    "\n",
    "First, create a `BFMatcher` object. The first parameter specifies the distance measure used for matching. L2-norm (Euclidean distance) is good for descriptors like SIFT and SURF, while binary string based descriptors like ORB, BRIEF, BRISK should use the Hamming distance, indicated by `cv2.NORM_HAMMING`. The second parameter enables cross-checking between two descriptors that match each other, i.e. only match those pairs whereby the $i$-th descriptor in set A has $j$-th descriptor in set B as the best match and vice-versa (both ways). This is a reliable technique to the \"ratio test\" method proposed in David Lowe's original SIFT specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxJJggI0vkSj"
   },
   "outputs": [],
   "source": [
    "# Create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Let's see what kind of information is in this list....\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVS-PiLYvkSs"
   },
   "source": [
    "Let's examine the first match..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4iqout0vkSu"
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "print(matches[num].distance,\"between descriptor\",matches[num].queryIdx,\"and\",matches[num].trainIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mO_6iCCvkS6"
   },
   "source": [
    "It's a list of `DMatch` objects. Read up more [here](http://docs.opencv.org/trunk/d4/de0/classcv_1_1DMatch.html) about `Dmatch`.\n",
    "\n",
    "It's a class for keypoint descriptor matches, and it contains a bunch of values -- query descriptor index, train descriptor index, train image index, and distance between descriptors.\n",
    "\n",
    "Next, sort the matched descriptors in the order of their distance. Use the in-built `sorted` function to sort the `matches` according to the distance attribute of the `DMatch` objects. A [lambda function](http://www.secnetix.de/olli/Python/lambda_functions.hawk) (a function that does not have a name, that can be created at run-time) is useful here to take the distance attributes from each object for comparison. Then, use `cv2.drawMatches()` to draw the first K matches between the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANWn-1kLvkS8"
   },
   "outputs": [],
   "source": [
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Prepare a composite image that can contain both images\n",
    "img_matches = np.zeros((img.shape[0]+img2.shape[0], img.shape[1]+img2.shape[1]))\n",
    "\n",
    "# Draw first K matches.\n",
    "K = 20\n",
    "img_matches = cv2.drawMatches(img,kps1,img2,kps2,matches[:K],img_matches,flags=4) # try flags=4\n",
    "\n",
    "plt.imshow(img_matches)\n",
    "plt.title('SIFT matching')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "sRFeOtUuvkTL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "And, our matching is done! Observe carefully to check if the matched keypoint descriptors correspond to the correct \"patch\" on both images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWfhE5N1vkTN"
   },
   "source": [
    "## Additional Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxZ-jUpvvkTO"
   },
   "source": [
    "**Q1**: A simplistic way of comparing images using SIFT descriptors is to sum up the distance between the top K matched descriptors. In other words, the distance between the top K pairs of matched descriptors are summed up to obtain a single value of dissimilarity. The smaller this value is, the more similar the two images that were matched. \n",
    "\n",
    "Here's a new image, featuring our very own KLCC \n",
    "<img src=\"klcc.jpg\" width=\"150\">\n",
    "Write some code in the function `findClosestPair` that can be used to compare pairs of images. Compare all 3 images (the two Notre Dame images and the KLCC image) against each other using matched SIFT descriptors to find out which two images are the closest to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOCgxRpkvkTQ"
   },
   "outputs": [],
   "source": [
    "# determine for yourself what input parameters to use.\n",
    "def findClosestPair(): \n",
    "    #Enter code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aor7DwJdvkTZ"
   },
   "source": [
    "**Q2**: Show the top K patches of two images that were matched. It will be interesting to see how *similar* or *different* the matched patches were, and why are they important interest points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eQEiVQLvkT9"
   },
   "outputs": [],
   "source": [
    "#Enter code here\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
