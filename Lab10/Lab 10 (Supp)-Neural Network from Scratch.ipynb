{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVr6xRwU51Q4"
   },
   "source": [
    "# Lab 10 (Supp): Neural Network from Scratch\n",
    "\n",
    "In this session, let's start of by attempting to build a neural network from scratch, so that you get an idea of what goes on under the hood of a neural network learner. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w-M7k_G51Q_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOanDldg51RP"
   },
   "source": [
    "What is a neural network? It is a machine learning technique that attempts to mimic the human brain by representing it with a series of connected nodes in different layers. A human brain consists of 100 billion cells called neurons, which are connected together by synapses. The synapses are in charge of propagating input stimuli as such that if there is sufficient strength in the synaptic inputs, the resulting (output) neuron will be fired or \"activated\". This is how signals are propagating through our brain cells that eventually lead to them activating some other body functions. \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*4-4XkuTZopk59wOV6E-RCg.jpeg\" width=400>\n",
    "\n",
    "We can model this process by creating a neural network on a computer. It's not necessary to model the biological complexity of the human brain at a molecular level, just its higher level rules and \"thinking\" capabilities. To start doing that, we will attempt to model just a single neuron, with three inputs and one output. Then we shall see how values are propagated both forward and backward inside a neural network.\n",
    "\n",
    "We are going to train the neuron to solve the following simple problem. These four examples will be our training set. From a quick glance, surely you can work out the pattern in the values below, and the answer for '?' is simple.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*nEooKljI8XbKQh4cFbZu1Q.png\" width=400>\n",
    "\n",
    "You might have noticed that the output is always equal to the value of the leftmost input column. Therefore the answer for the ‘?’ should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhwQg-Q-51RU"
   },
   "outputs": [],
   "source": [
    "training_set_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = np.array([[0, 1, 1, 0]]).T\n",
    "print(training_set_inputs)\n",
    "print(training_set_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41sRdoVi51Ru"
   },
   "source": [
    "### Training a Neural Network\n",
    "\n",
    "So, how do we teach our neuron to answer the question correctly? We will give each input a weight, which can be a positive or negative number. An input with a large positive weight or a large negative weight, will have a strong effect on the neuron’s output. Likwise, a weight that is close to zero would have not much effect. This are also known as the \"activations\" of the neurons. Before we start, let us set each weight to a random number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEv5z0Zh51Ry"
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator, so it generates the same numbers\n",
    "# every time the program runs.\n",
    "np.random.seed(1)   # normally we get the seed from the machine clock\n",
    "\n",
    "synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "print(synaptic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2n4Vi8h51SE"
   },
   "source": [
    "Then, we begin the training process, which follows these steps:\n",
    "\n",
    "1. Take the inputs from a training set example, adjust them by the weights, and pass them through a \"special formula\" to calculate the neuron's output.\n",
    "2. Calculate the error, which is the difference between the neuron's output and the desired output in the training set example.\n",
    "3. Depending on the direction of the error, adjust the weights slightly.\n",
    "4. Repeat this process for many times (hundreds or thousands of times).\n",
    "\n",
    "This process involve a **feed-forward** step (step 1) and a **back propagation** step (step 3). Eventually the weights of the neurons will reach an optimum level for the training set, where it will no longer change by much. \n",
    "\n",
    "### The \"Special Formula\"\n",
    "\n",
    "The so-called special formula for calculating the neuron’s output can be calculated by first taking the weighted sum of the neuron's inputs:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_i w_i x_i = w_1 x_1 + w_2 x_2 + w_3 x_3\n",
    "\\end{align}\n",
    "\n",
    "Next, we want to normalise this value so that it lies between 0 and 1. In fact, it would be good if the function is able to push the values to the extremas of the range between 0 and 1 so that we can emphasize on the strong activations. For this, we use a non-linear activation function called the Sigmoid function.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*sK6hjHszCwTE8GqtKNe1Yg.png\" width=400>\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxpkN3pD51SJ"
   },
   "outputs": [],
   "source": [
    "# The Sigmoid function, which describes an S shaped curve.\n",
    "# We pass the weighted sum of the inputs through this function to\n",
    "# normalise them between 0 and 1.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMB4cgPw51SW"
   },
   "source": [
    "So, by substituting the first equation into the second, the final formula for the output (activation) of the neuron is:\n",
    "\n",
    "\\begin{align}\n",
    "a = f(\\sum_i w_i x_i) = \\frac{1}{1+e^{-(\\sum_i w_i x_i)}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCnjB4jE51Sa"
   },
   "outputs": [],
   "source": [
    "output = sigmoid(np.dot(training_set_inputs, synaptic_weights))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZbgOP4M51Su"
   },
   "source": [
    "### Adjusting the weights\n",
    "\n",
    "With the calculated output and the desired output (based on training data), we can now find the error which will tell us how far is the current output from the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNrr7Mc451Sy"
   },
   "outputs": [],
   "source": [
    "# Calculate the error (The difference between the desired output\n",
    "# and the predicted output).\n",
    "error = training_set_outputs - output\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_KMvtDX51TD"
   },
   "source": [
    "To make an adjustment to the weights based on this error, we can first multiply the error with the input which is either 0 or 1. This makes it proportional to the amount of the error. Then, we go on to multiply by the gradient of the activation function (which is a Sigmoid). \n",
    "\n",
    "The intuition behind doing this, is that when the output is a large positive or large negative number, it signifies the strength or confidence of the neuron. Based on the Sigmoid curve, large numbers (on both ends) will have a small or shallow gradient. If the neuron is confident that the existing weight is correct, it will not adjust it by very much. So, multiplying by the Sigmoid curve gradient achieves this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9AEsNl651TJ"
   },
   "outputs": [],
   "source": [
    "# The derivative of the Sigmoid function.\n",
    "# This is the gradient of the Sigmoid curve.\n",
    "# It indicates how confident we are about the existing weight.\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBjlkTXN51Ta"
   },
   "outputs": [],
   "source": [
    "# Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "# This means less confident weights are adjusted more.\n",
    "# This means inputs, which are zero, do not cause changes to the weights.\n",
    "\n",
    "print(error * sigmoid_derivative(output))\n",
    "adjustment = np.dot(training_set_inputs.T, error * sigmoid_derivative(output))\n",
    "print(adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kerPqTyr51Tu"
   },
   "outputs": [],
   "source": [
    "old_weights = synaptic_weights.copy()\n",
    "\n",
    "synaptic_weights += adjustment\n",
    "print(\"Old weights:\\n\",old_weights)\n",
    "print(\"New weights:\\n\",synaptic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE49cFwO51UD"
   },
   "source": [
    "We can see that the weights have changed. One iteration (also known as epoch) has passed when all training examples have passed through the network, updating the weights by back-propagation.\n",
    "\n",
    "### Iterate it\n",
    "\n",
    "What we need to do now is to iterate the process and see if the weights converge at some stable values. The proper way of knowing this is to calculate the network loss for each iteration. In this example, the loss is basically the sum of squared error between the predicted and desired output. Sometimes, the average loss is also used because it provides the intuition of how far is each sample (on average) from the ground truth after the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWyzXzl_51UG"
   },
   "outputs": [],
   "source": [
    "synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "loss = []\n",
    "for iteration in range(1000):\n",
    "    output = sigmoid(np.dot(training_set_inputs, synaptic_weights))\n",
    "    error = training_set_outputs - output\n",
    "    synaptic_weights += np.dot(training_set_inputs.T, error * sigmoid_derivative(output))\n",
    "    rss = np.sum(error**2)\n",
    "    loss.append(rss)\n",
    "    \n",
    "    print(\"Iteration\",iteration,\" loss=\", rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ibiq4lnz51Ud"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1000), loss,'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8QZv3E551Uz"
   },
   "source": [
    "Finally, let's test out the model learned by the neural network with a new test input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YoWnTJ851U2"
   },
   "outputs": [],
   "source": [
    "test_input = np.array([1, 0, 0])\n",
    "test_output = sigmoid(np.dot(test_input, synaptic_weights))\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G44bonS_51VE"
   },
   "source": [
    "The answer is close to 1, which is correct. \n",
    "By examining the final weights learned from the neural network, we can see that the first weight is a strong positive number, indicating that it contributes the most to the decision making. Recall again that we observed the first column being similar to the output value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iG3Uf2mS51VH"
   },
   "outputs": [],
   "source": [
    "print(\"Final weights:\\n\",synaptic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmDA_KTP51VV"
   },
   "source": [
    "## Problem case: Lack of Features\n",
    "\n",
    "Now let's look at a slightly more challenging set of training data. \n",
    "<table>\n",
    "    <tr><th colspan=3>Input</th><th>Output</th>\n",
    "       <tr><td>0</td><td>0</td><td>1</td><td>0</td></tr>\n",
    "       <tr><td>0</td><td>1</td><td>1</td><td>1</td></tr>\n",
    "       <tr><td>1</td><td>0</td><td>1</td><td>1</td></tr>\n",
    "       <tr><td>1</td><td>1</td><td>1</td><td>0</td></tr>\n",
    "    </table>\n",
    "So, what's the pattern here? The output appears to be completely unrelated to column three, which is always 1. However, columns 1 and 2 provide more clarity. If either columns 1 or 2 are a 1 (but not both!) then the output is a 1. If either columns are a 0 then the output is a 0. The third column is as good as redundant, so we are hoping for the first two columns to help make correct predictions. \n",
    "\n",
    "This is considered a \"non-linear\" pattern because there is no direct one-to-one relationship between the input and output. Instead, there is a one-to-one relationship between a combination of inputs, namely columns 1 and 2. This is going to be challenging. There is a lack of features (or rather, useful features). \n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/08/knn_kaggle_dogs_vs_cats_sample.jpg\" width=450>\n",
    "\n",
    "Image recognition has the similar problem. Given a bunch of images of dogs or cats (assume they are identical in size), we will find that no individual pixel position would directly correlate with the presence of a dog or cat. The pixels are as good as random from a purely statistical point of view. However, certain combination of pixels are not entirely random, namely the combinations that form certain body parts of the cat or dog might be of good use. So, we need to find a \"higher level\" correlation between these combination of pixels with the output values.\n",
    "\n",
    "### Strategy\n",
    "\n",
    "In order to combine pixels into something that can then have a one-to-one relationship with the output, we need to add another layer. The first layer will combine the inputs, and the second layer will then map them to the output with the output of the first layer as input. This new layer that is neither an input nor an output layer, is usually known as a *hidden layer*, because it is not observable in any sense, but derives a relationship between the input and output layers.\n",
    "\n",
    "In terms of weight updating, this neural network will need to update the second layer of weights that maps to the output, and also update the first layer of weights to be better at producing it from the input! So there shall two weight updates, linked to each other.\n",
    "\n",
    "Let's start by creating the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x51OxEZv51Vb"
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "[0,0,1],\n",
    "[0,1,1],\n",
    "[1,0,1],\n",
    "[1,1,1]]) \n",
    "y = np.array([\n",
    "[0],\n",
    "[1],\n",
    "[1],\n",
    "[0]])\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x7wCnnL51Vr"
   },
   "source": [
    "Then, we initialize the two weight matrices randomly. Their shapes need to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39yo4IPR51Vu"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "weight0 = 2*np.random.random((3,5)) - 1\n",
    "weight1 = 2*np.random.random((5,1)) - 1\n",
    "print(weight0)\n",
    "print(weight1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEBYb2Ji51WA"
   },
   "source": [
    "The feedforward operation involves passing the inputs through all layers, until the predicted output is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQa735MH51WD"
   },
   "outputs": [],
   "source": [
    "# Feed forward through layers 0, 1, and 2\n",
    "layer0 = X\n",
    "layer1 = sigmoid(np.dot(X,weight0))\n",
    "layer2 = sigmoid(np.dot(layer1,weight1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D6VomQV51WP"
   },
   "outputs": [],
   "source": [
    "# Then, calculate the error at the end of network : how far are we from target\n",
    "l2_error = y - layer2\n",
    "print(l2_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAlRSTAO51Wf"
   },
   "source": [
    "Back-propagaton step starts once we have the error at the final layer. The amount of adjustment needs to be calculated in the last layer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkGwvlvb51Wi"
   },
   "outputs": [],
   "source": [
    "l2_delta = l2_error*sigmoid_derivative(layer2)\n",
    "print(l2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zfXz-QS51Wv"
   },
   "source": [
    "Then, it trickles over from the 2nd layer to the 1st layer. This is to find out how much each layer1 value contribute to the layer2 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUiLbH_m51Wy"
   },
   "outputs": [],
   "source": [
    "l1_error = np.dot(l2_delta, weight1.T)\n",
    "print(l1_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggYWKXYt51W_"
   },
   "outputs": [],
   "source": [
    "# this is the amount of adjustment needed on layer 1\n",
    "l1_delta = l1_error * sigmoid_derivative(layer1)\n",
    "print(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAbA6-Mn51XO"
   },
   "outputs": [],
   "source": [
    "# copy old weights first\n",
    "old_weight1 = weight1.copy()\n",
    "old_weight0 = weight0.copy()\n",
    "\n",
    "# update weights with the adjustment\n",
    "weight1 += np.dot(layer1.T, l2_delta)\n",
    "weight0 += np.dot(layer0.T, l1_delta)\n",
    "\n",
    "# print to check if there are changes\n",
    "print(\"Old weight 1\\n\",old_weight1)\n",
    "print(\"New weight 1\\n\",weight1)\n",
    "print(\"Old weight 0\\n\",old_weight0)\n",
    "print(\"New weight 0\\n\",weight0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RD67lQOL51Xc"
   },
   "source": [
    "**Q1**: In order to verify that the neural network is able to train correctly, collect and compile all relevant code above, and put them into a loop. As before, keep track of the network loss as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-UARX8X51Xe"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de9Fcz-h51Xn"
   },
   "source": [
    "**Q2**: Plot the loss vs. epoch curve. It should show some form of convergence towards a minimum level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2ZCFg0i51Xt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnPHfHY051X1"
   },
   "source": [
    "**Q3**: Add a *learning rate* to your weight updating code. Find a good value for it such that it will give us the lowest possible loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4O0DjEd51X4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovCJVW0t51YB"
   },
   "source": [
    "Once your neural network has learned sufficiently well, take a look at the weights (both layers) and see if you can see anything interesting happening that might provide you with some idea of what was learned\n",
    "\n",
    "The neural network flavour that you have just created is called a **Multilayer Perceptron (MLP)**. MLP is a class of feedforward artificial neural network which consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a non-linear activation function. MLP utilizes a supervised learning technique called back-propagation for training."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CDS6334",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
