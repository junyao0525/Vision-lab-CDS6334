{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwdtMrTfvuiq"
   },
   "source": [
    "# Lab09 - BOW Representation for Image Retrieval\n",
    "### CDS6334 Visual Information Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsagM6bivuiu"
   },
   "source": [
    "This lab puts you through the process of generating visual words by constructing a *visual vocabulary* based on local features such as SIFT, and proceed to create *bag of words (BOW) representation* for each image. You will also try your hand at implementing the tf-idf weighting scheme on the BOW representation, plus create some nice visualization of the visual word patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfVxSAe3vuix"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RLIepSmvujJ"
   },
   "source": [
    "## Generating visual vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MheFMQ1bvujL"
   },
   "source": [
    "To generate a good visual vocabulary that generalizes well, it is good to use a set of images, rather than just a single image. In this exercise, we will use a small set of 10 images to demonstrate this task.\n",
    "\n",
    "**Revisting subplots.** Let's revise again on how subplots can be created using the matplotlib function `subplots` (we will be needing subplots very often in this lab). An interesting technique of cycling through all the axes objects in a figure of subplots is to use the `ravel` function which allows us to access each subplot using a single index number. This is more convenient than using row and column indices to define a subplot. Bear in mind that the single index number cycles through the subplots following [row-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC77QpQdvujN"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(12, 3))\n",
    "fig.subplots_adjust(hspace = .2, wspace=.1)    # hspace and wspace defines the horizontal and vertical gap between subplots\n",
    "axs = axs.ravel()                              # \"unravel\" these subplots into a \"vector\"\n",
    "for i in range(10):\n",
    "    filename = \"dataset/\" + str(i) + \".jpg\"\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(filename[8:])\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "    \n",
    "#plt.savefig('wildlife.png', frameon=False, bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHR3feuGvuje"
   },
   "source": [
    "Occasionally, you might think this looks like a nice arrangement with some nice labeling. How can we save this figure up? Go back to the previous set of code and uncomment the line that executes the function `savefig`. You may use your own choice of filename and a number of possible image file extensions (see [here](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.savefig.html)). The parameters `frameon=False` enables transparency for the background of the figure, and `bbox_inches='tight'` creates a tight bounding box for the figure (less whitespace around it).\n",
    "\n",
    "**Note**: You cannot call `savefig` AFTER you have called `show`, as the function `show` already dumps the graphic to your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rl3n7HZvujh"
   },
   "source": [
    "### Local feature extraction\n",
    "\n",
    "So, let's now get back to the feature extraction part.\n",
    "\n",
    "Let's get the SIFT descriptors from each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcTHhzHovujj"
   },
   "outputs": [],
   "source": [
    "# create some lists\n",
    "imgs = []\n",
    "feat = []   \n",
    "\n",
    "for i in range(10):\n",
    "    filename = \"dataset/\" + str(i) + \".jpg\"\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.1)\n",
    "    kps, des = sift.detectAndCompute(gray, None)\n",
    "    \n",
    "    imgs.append(img)\n",
    "    feat.append((kps, des))     # list contains a tuple of two arrays -- keypoint, descriptor\n",
    "    print('Computing SIFT for image #%d'%(i))   # some verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIUgOnsnvujw"
   },
   "outputs": [],
   "source": [
    "# check: print number of descriptors for each image\n",
    "for i in range(10):\n",
    "    print(feat[i][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCVumx5jvuj-"
   },
   "source": [
    "Let's put the previous code for visualizing descriptors into an easy function to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdMrWgO2vukA"
   },
   "outputs": [],
   "source": [
    "def showDescriptors(img, kps, flag=4):\n",
    "    imm = np.zeros((img.shape))    \n",
    "    imm = cv2.drawKeypoints(img, kps, imm, flags=flag); \n",
    "    imm = cv2.cvtColor(imm, cv2.COLOR_RGB2BGR)\n",
    "    plt.imshow(imm)\n",
    "    plt.show()\n",
    "    \n",
    "# show SIFT descriptors for a certain image\n",
    "imageNum = 9\n",
    "showDescriptors(imgs[imageNum], feat[imageNum][0], flag=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y24FEMGmvukM"
   },
   "source": [
    "### Clustering descriptors to build visual words\n",
    "\n",
    "To construct the visual vocabulary, we need to first compile together all the SIFT descriptors that we found from all 10 images (Note: This is considered a small amount actually. In most cases we need to build the visual vocabulary from thousands of images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCsGDaXsvukP"
   },
   "outputs": [],
   "source": [
    "# since feat contain a list of tuples, a cool way to \"unpack\" or \"unzip\" a bunch of tuples into \n",
    "# separate lists is by using the zip function with an asterisk *\n",
    "loc, des = list(zip(*feat))\n",
    "\n",
    "# the single line above is similar to performing these two list comprehension operations\n",
    "#loc = [item[0] for item in feat]\n",
    "#des = [item[1] for item in feat]\n",
    "\n",
    "# stack the lists of descriptors vertically (since descriptors are in rows)\n",
    "alldes = np.vstack(des)\n",
    "print(alldes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peZS1yVUvukf"
   },
   "source": [
    "Next, we can cluster these descriptors using k-means clustering. The general rule of thumb is that the number of clusters (k) should be much lesser than the total number of descriptors (so that we don't end up with a sparse histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6QoChTlvukh"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# k is the number of clusters\n",
    "k = 50\n",
    "alldes = np.float32(alldes)      # convert to float, required by kmeans and vq functions\n",
    "e0 = time.time()\n",
    "codebook, distortion = kmeans(alldes, k)\n",
    "code, distortion = vq(alldes, codebook)\n",
    "e1 = time.time()\n",
    "print(\"Time to build {}-cluster codebook from {} images: {} seconds\".format(k,alldes.shape[0],e1-e0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye2LwRZJvuku"
   },
   "source": [
    "### Detour: Pickling objects\n",
    "\n",
    "Let's detour a little to look at how to store Python objects by serializing them. We call this \"pickling\" (like how you pickle vegetables to keep them for a long time...). Considering that you have learned a codebook from the SIFT descriptors. This codebook can be stored away for later use, for deployment at the user/app end. If we have a huge amount of data to learn the codebook (which might be time-consuming), we should not be running this over and over, but instead, have it pickled after the heavy computations in the construction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnSXAthRvukw"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(codebook, open(\"codebook.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0eNKstBvuk7"
   },
   "outputs": [],
   "source": [
    "del codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setL5vojvulF"
   },
   "outputs": [],
   "source": [
    "codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzWa44avvule"
   },
   "source": [
    "To load back the pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxelHzJavulg"
   },
   "outputs": [],
   "source": [
    "codebook = pickle.load( open( \"codebook.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGqgfU1Pvulp"
   },
   "outputs": [],
   "source": [
    "print(codebook.shape)\n",
    "print(codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGEHLVjrvul1"
   },
   "source": [
    "Now we are ready to continue where we left off..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0p2HkhgUvul3"
   },
   "source": [
    "### Visualizing visual word patches\n",
    "Let's attempt to visualize the patches that belong to a particular cluster. The cluster that is the largest (with the most descriptor patches assigned to it) can be found by this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA9PWeeYvul4"
   },
   "outputs": [],
   "source": [
    "out = np.histogram(code, bins=k)\n",
    "\n",
    "# shows the distribution of the descriptors belonging to the k-th visual word\n",
    "plt.bar(np.arange(k), out[0]), plt.show()\n",
    "\n",
    "# find the index of the visual word that has the largest number of (descriptor) members \n",
    "print(np.argmax(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nyv2qgIqvumF"
   },
   "source": [
    "The following code is to visualize patches that belong to a certain visual word (we select the one with the largest cluster so that we have a chance of seeing more patches). As example, we only take descriptors that belong to the first image (image 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdoaWgBZvumH"
   },
   "outputs": [],
   "source": [
    "# select the visual word with the largest cluster, and find its descriptors that belong to image 0\n",
    "im = 0\n",
    "c = 5                                               # c can take values between 0 and k\n",
    "code0, distortion = vq(feat[im][1], codebook)       # for ease, do VQ for descriptors of image 0\n",
    "rows = np.where(code0 == c)                         # find descriptors that belong to c-th word\n",
    "nrows = len(rows[0])\n",
    "print(\"Number of descriptors of word %d in image %d: %d\"%(c,im,nrows))\n",
    "\n",
    "imPerLine = 16          # limit each line to only show a number of patches\n",
    "lines = nrows//imPerLine+1\n",
    "fig, axs = plt.subplots(lines, imPerLine)\n",
    "fig.set_size_inches((18,4))\n",
    "fig.subplots_adjust(hspace = .1, wspace=.1)\n",
    "axs = axs.ravel()\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "# access the keypoints of these descriptors\n",
    "for r in np.arange(nrows):\n",
    "    imgfeat = feat[im][0]    # get the image's features keypoints\n",
    "    desnum = rows[0][r]\n",
    "    winsize = np.int32(np.round(imgfeat[desnum].size))\n",
    "    y = np.int32(imgfeat[rows[0][r]].pt[1])                   \n",
    "    x = np.int32(imgfeat[rows[0][r]].pt[0])\n",
    "    if (y+winsize > imgs[0].shape[0] or x+winsize > imgs[0].shape[1]):\n",
    "         continue\n",
    "    \n",
    "    patch = imgs[0][y-winsize:y+winsize, x-winsize:x+winsize, :]   \n",
    "    patch = cv2.resize(patch, (15, 15), interpolation = cv2.INTER_AREA)\n",
    "   \n",
    "    axs[r].imshow(patch, aspect='equal')\n",
    "    axs[r].set_xticks([])\n",
    "    axs[r].set_yticks([])\n",
    "\n",
    "# this is to turn off the axis for remaining subplots that are unused\n",
    "for r2 in np.arange(nrows,imPerLine*lines):    \n",
    "    axs[r2].axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgXSYUyLvumR"
   },
   "source": [
    "**Observation note**: If you observe that the patches for each visual word contain a large variety of views, that could mean that the number of clusters used earlier to build the vocabulary is too small. If you see that different clusters have patches that are similar, that could mean that the number of clusters is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmH2NtQ7vumT"
   },
   "source": [
    "## Bag-of-words (BOW) representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhBKp3VbvumV"
   },
   "source": [
    "After constructing the visual vocabulary, we can now extract the Bag-of-words (BOW) representation by taking the normalized histogram of visual word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0lY0k73vumY"
   },
   "outputs": [],
   "source": [
    "bow = list();\n",
    "for i in np.arange(10):\n",
    "    code, distortion = vq(feat[i][1], codebook)\n",
    "    bowhist = np.histogram(code, k, density=True)\n",
    "    bow.append(bowhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1ZC9WHjvuml"
   },
   "source": [
    "This is how the BOW histogram for image 0 looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHIDvEMAvumn"
   },
   "outputs": [],
   "source": [
    "plt.bar(bow[0][1][1:], bow[0][0]); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-O4AYBOvumy"
   },
   "source": [
    "Let's visualize the BOW histogram of all 10 images side-by-side (making sure y-axis is fixed)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rMLPxJCvum0"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(12, 3), facecolor='w', edgecolor='w')\n",
    "fig.subplots_adjust(hspace = .1, wspace=.1)\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    filename = \"dataset/\" + str(i) + \".jpg\"\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(filename[8:])\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "for i in range(10,20):\n",
    "    axs[i].bar(bow[i-10][1][1:], bow[i-10][0]); \n",
    "    axs[i].set_xlim((0,k)), axs[i].set_ylim((0, 0.15))\n",
    "    axs[i].set_xticks([]), axs[i].set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3NEFuI1vum9"
   },
   "source": [
    "### Measuring distance between BOW features\n",
    "\n",
    "In order to perform a retrieval or classification task, there must be a means to match the features extracted from these images. The cosine distance between two vectors $u$ and $v$, is defined as $$1-\\frac{u \\cdot v}{\\|u\\|\\|v\\|}$$ where $u \\cdot v$ is the dot product between the two vectors.\n",
    "\n",
    "Let's consider first image `'0.jpg'` as the query image. We now write a loop to find the cosine distance from the query image to the other 9 images. We also create labels to identify the relevant classes involved ('0' for tiger, '1' for turtle), and then pack both the labels and the distancs in to a tuple, appending it to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKX3B7-bvunA"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "results = list();\n",
    "for i in np.arange(10):\n",
    "    d = distance.cosine(bow[0][0], bow[i][0])\n",
    "    results.append((labels[i], d))     # place the label and distance in a tuple, append to list of tuples\n",
    "\n",
    "_ = [print(r) for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UblRyKZvunJ"
   },
   "source": [
    "Obviously, the first image matches itself (the first image) perfectly, and hence the distance measured is 0. For all other images, a distance value between 0 and 1 is produced. The smaller this value, the closer it is to the query image, on the basis of comparing their BOW-SIFT features.\n",
    "\n",
    "To sort the distances in the `results` tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiNk7j7tvunL"
   },
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key=lambda item: item[1])     # the sorting key takes the second value of the tuple\n",
    "_ = [print(sr) for sr in sorted_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qlipb0gvunV"
   },
   "source": [
    "What we can observe here is that, excluding the first match (which is the same image anyway), the top 4 matches gives us images with the following labels: 0, 0, 0, 0 -- all 4 are tigers. Excellent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qqoqWaTvunX"
   },
   "source": [
    "**Q1**: What if another query image (try *q1.jpg* or *q2.jpg*) is used as the query image instead? Would you still be getting such perfect results? Test this out and see.\n",
    "<table><tr><td><img src=\"q1.jpg\" width=\"200\"></td><td><img src=\"q2.jpg\" width=\"200\"></td></tr></table>\n",
    "\n",
    "Complete the code in function `retrieveImgs` to output the retrieval results given an image filename. Do keep in mind that the query image needs to undergo the similar processing steps: SIFT extraction, vector quantization using the codebook constructed earlier (do not need to rebuild) and finally, matching against the images in the gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfTtFXn6vunZ"
   },
   "outputs": [],
   "source": [
    "def retrieveImgs(filename, bow):\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # extract SIFT features\n",
    "\n",
    "    \n",
    "    # load the pickled codebook, apply vq to the features\n",
    "\n",
    "    \n",
    "    # plot BOVW histogram for the image\n",
    "\n",
    "    \n",
    "    # compute cosine distance between the query image and each of the image in the dataset\n",
    "    # store the distance in the variable \"results\" and return it to the calling function \n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "file = 'q2.jpg'\n",
    "results = retrieveImgs(file, bow)\n",
    "\n",
    "# print out the results, sorted by distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TXU6EhDvunj"
   },
   "source": [
    "## Performance Metrics\n",
    "\n",
    "### Precision and Recall\n",
    "Precision and Recall are used to evaluate various types of tasks, from detection to retrieval. General precision and recall are calculated by:<br>\n",
    "$$\\begin{align}Precision &= \\frac{\\text{TP}}{\\text{TP + FP}} = \\frac{\\text{Relevant Images}}{\\text{Total Images Retrieved}}\\\\\n",
    "Recall &= \\frac{\\text{TP}}{\\text{TP + FN}} = \\frac{\\text{Relevant Images}}{\\text{Total Relevant Images in Database}}\\end{align}$$\n",
    "\n",
    "However, this evaluation disregards the ranking of the images retrieved. In any retrieval tasks, the ranking indicates the quality of the retrieval algorithm. Therefore, the top-$K$ of the retrieved results should be considered, and for this we will need to calculate precision and recall at each retrieved result. \n",
    "\n",
    "### Precision@K\n",
    "\n",
    "Consider the example in **Q1**, with the assumption that there are 5 relevant images, and the top 10 images that were retrieved ($K=10$) to match a tiger query image (1), are ranked as follows:\n",
    "$$1, 1, 0, 1, 0, 1, 0, 0, 1, 0$$\n",
    "In this case, we are only interested in $K$ number of retrieved results. \n",
    "We can calculated **Precision@K** by the same equation above but only at **K** amount of retrievals. For instance, if we were to find the Precision@1 till Precision@5 (i.e. when the top 1 till top 5 images are retrieved), we will have each of the precision as follows:\n",
    "$$\\begin{align} \\text{Precision@1} & = \\frac{1}{1} = 1 \\\\ \n",
    "\\text{Precision@2} & = \\frac{2}{2} = 1 \\\\ \n",
    "\\text{Precision@3} & = \\frac{2}{3} = 0.67 \\\\\n",
    "\\text{Precision@4} & = \\frac{3}{4} = 0.75 \\\\\n",
    "\\text{Precision@5} & = \\frac{3}{5} = 0.6\\end{align}$$ \n",
    "This is repeated until all top-10 retrieved images are considered.\n",
    "\n",
    "### Recall@K\n",
    "Likewise, the **Recall@K** can be determined using the total relevant images in the database that is 5, as follows:\n",
    "$$\\begin{align} \\text{Recall@1} & = \\frac{1}{5} = 0.2 \\\\ \n",
    "\\text{Recall@2} & = \\frac{2}{5} = 0.4 \\\\ \n",
    "\\text{Recall@3} & = \\frac{2}{5} = 0.4 \\\\\n",
    "\\text{Recall@4} & = \\frac{3}{5} = 0.6 \\\\\n",
    "\\text{Recall@5} & = \\frac{3}{5} = 0.6\\end{align}$$. \n",
    "\n",
    "These pairs of **@K** values are used to plot the Precision-Recall curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxz4jGKTvunl"
   },
   "source": [
    "**Q2**: Write some functions to calculate the ranked Precision@K and Recall@K of a query image given K number of images retrieved. After that, use these functions to plot the Precision-Recall curve by generating many pairs of Precision and Recall values. Note that the Precision@K function will be useful for calculating the AP in Q3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9bcNQLXvunq"
   },
   "outputs": [],
   "source": [
    "#Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5xqTLikJK38"
   },
   "source": [
    "### Average Precision\n",
    "\n",
    "**Average Precision** (AP) is the average of the precision value obtained for the set of top $K$ images existing after each relevant image is retrieved. It considers the rank of the retrieved results when calculating the precision score: \n",
    "$$AP=\\frac{1}{|R|}\\sum_{k=1}^{K}Prec(k)\\cdot rel(k)$$ \n",
    "where $R$ is the total number of relevant images, $Prec(k)$ is the precision at top $k$ documents and $rel(k)$ indicates the relevance of the $k$-th retrieved document -- 1 if relevant, 0 otherwise.\n",
    "\n",
    "Using the same example, where 1 indicates the relevant tiger images retrieved:\n",
    "$$1, 1, 0, 1, 0, 1, 0, 0, 1, 0$$\n",
    "The Average Precision can be calculated as follows: \n",
    "$$\\begin{align} AP & = \\frac{1}{5}\\biggl[\\biggl(\\frac{1}{1}\\cdot 1 \\biggr) + \\biggl(\\frac{2}{2}\\cdot 1 \\biggr) + \\biggl(\\frac{2}{3}\\cdot 0 \\biggr) + \\biggl(\\frac{3}{4}\\cdot 1 \\biggr) + \\biggl(\\frac{3}{5}\\cdot 0 \\biggr) \\\\ &+ \\biggl(\\frac{4}{6}\\cdot 1 \\biggr) + \\biggl(\\frac{4}{7}\\cdot 0 \\biggr) + \\biggl(\\frac{4}{8}\\cdot 0 \\biggr) + \\biggl(\\frac{5}{9}\\cdot 1 \\biggr) + \\biggl(\\frac{5}{10}\\cdot 0 \\biggr)\\biggr] \\\\\n",
    "& = \\frac{1}{5}\\biggl[\\biggl(\\frac{1}{1}\\biggr) + \\biggl(\\frac{2}{2}\\biggr) + \\biggl(\\frac{3}{4}\\biggr) + \\biggl(\\frac{4}{6}\\biggr) + \\biggl(\\frac{5}{9}\\biggr) \\biggr] \\\\ & = 0.794 \\end{align}$$ \n",
    "\n",
    "AP provides a realistic interpretation for applications that require ranked retrieval tasks since the retrieved results that are closer to the top (or the so-called \"top search hits\" for web search results) carry more weight than those further lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uizVjVNOvun5"
   },
   "source": [
    "**Q3**: Write a function to calculate the Average Precision (AP) of a query image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnSmnFmxvun7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "#Example\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "avg = average_precision_score(y_true, y_scores)  \n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K561V-QEJK39"
   },
   "outputs": [],
   "source": [
    "#Enter your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn1HFew3JK39"
   },
   "source": [
    "### Mean Average Precision\n",
    "\n",
    "mAP is the *average* of Average Precisions for all queries:\n",
    "$$mAP = \\frac{1}{Q}\\sum_{q=1}^{Q}AP_q$$\n",
    "where $Q$ is the total number of queries. This gives an overview of all retrieval performance when given various queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa82fK6fvuoE"
   },
   "source": [
    "**Q4**: Write another function to calculate the mean Average Precision (mAP) metric, which will represent the overall performance on a full set of query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elDeSalGvuoG"
   },
   "outputs": [],
   "source": [
    "#Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtUz9J5BvuoO"
   },
   "source": [
    "## Additional Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2i76jMRvuoQ"
   },
   "source": [
    "**Q1**: Can *local* (patch-level) **color descriptors** be used in place of SIFT descriptors to construct a Bag-of-words (BOW) feature representation? The only matter needing consideration is where your keypoints will come from. You can choose to use back the keypoints extracted from SIFT (SIFT uses a set of DoG blob detector at various scales/octaves), or you can opt to use Harris corners as keypoints. After that, obtain the descriptor from a fixed-size local window surrounding the keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhRcAfFWvuoR"
   },
   "outputs": [],
   "source": [
    "#Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "m6fLyp4Avuob",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Q2**: Proof by computation that the Average Precision (AP) score approximates to the area under the Precision-Recall (PR) curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLI_livtvuod"
   },
   "outputs": [],
   "source": [
    "#Enter your code here\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
